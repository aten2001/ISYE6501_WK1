---
title: "ISYE6501 Week1 Homework"
author: "Keh-Harng Feng"
date: "May 17, 2017"
output: 
    bookdown::html_document2:
        fig_caption: TRUE
        toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, cache = TRUE)

# importing required libraries.
library('knitr')
library('kernlab')
```

# Question 1
Describe a situation or problem from your job, everyday life, current events, etc., for which a classification model would be appropriate. List some (up to 5) predictors that you might use.

**Answer**: 

I recently resettled in another city and had to choose what to do with my belongings prior to the move. This process can be modelled as a classification problem with four possible responses for each item I own: `take it with me`, `let the mover deal with it`, `discard`, and `sell`. Five of the strongest predictors in my own decision process are:

`Monetary Value`

`Sentimental Value`

`Portability`

`Necessity`

`Importance`

The first four are pretty self-explanatory. `Importance` represents the significance of information or security possessed by a particular object. For example, my social security card does not have a lot of direct monetary value or sentimental value, yet it poses great security concerns and therefore should be taken by myself.

# Question 2
The files credit_card_data.txt (without headers) and credit_card_data-headers.txt (with headers) contain a dataset with 654 data points, 6 continuous and 4 binary predictor variables. It has anonymized credit card applications with a binary response variable (last column) indicating if the application was positive or negative. The dataset is the “Credit Approval Data Set” from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Credit+Approval) without the categorial variables and without data points that have missing values.

## Question 2.1
Using the support vector machine function ksvm contained in the R package kernlab, find a good classifier for this data. Show the equation of your classifier, and how well it classifies the data points in the full data set. (Don’t worry about test/validation data yet; we’ll cover that topic soon.)

**Answer**:

```{r searchC_compute}
require('kernlab')

# Compute accuracy
helper <- function(data, c_val) {
    rows = nrow(data)
    cols = ncol(data)
    
    model <- ksvm(as.matrix(data[,1:cols-1]), as.factor(data[,cols]), type="C-svc",kernel="vanilladot",C=c_val,scaled=TRUE)
    
    pred <- predict(model, data[,1:cols-1])
    
    ans <- list(c_val = c_val, acc = sum(pred == data[,cols])/rows, model = model)
    
    return(ans)
}

# data: data frame to be used for model building. Last column is the response.
# range: floating point in the form c(lower, upper). Search C in [lower, upper]. lower > 0!
# grid_n: number of grid points (including start and end points). Number of intervals = grid_n - 1.
# level: int indicating which level of grid search to perform (set to start at 1 initially)
searchC <- function(data, range = c(1, 101), grid_n, level = 1) {
    
    # grid search for maximum acc & corresponding index
    c_vals <- seq(range[1], range[2], length.out = grid_n)
    
    max_acc <- 0
    max_ind <- 1
    ans <- list()
    for (i in 1:grid_n) {
        ans <- helper(data, c_vals[i])
        
        if (ans$acc[1] > max_acc) {
            max_ind <- i
            max_acc <- ans$acc
        }
    }
    
    if (level < 2) {
        
        if (max_ind == 1) {
            range <- c(c_vals[max_ind], c_vals[max_ind + 2])
        } else if (max_ind == length(c_vals)) {
            range <- c(c_vals[max_ind - 2], c_vals[max_ind])
        } else {
            range <- c(c_vals[max_ind - 1], c_vals[max_ind + 1])
        }
        
        return(searchC(data, range, grid_n, level+1))
        
    } else {
        
        return(ans)
    }
}
```

The data is loaded with the following code:
```{r ksvm, echo = TRUE}
if (!file.exists('credit_card_data-headers.txt')) {
    download.file('https://d37djvu3ytnwxt.cloudfront.net/assets/courseware/v1/e39a3df780dacd5503df6a8322d72cd2/asset-v1:GTx+ISYE6501x+2T2017+type@asset+block/credit_card_data-headers.txt', 'credit_card_data-headers.txt')
}

credit_data <- read.table('credit_card_data-headers.txt', header = TRUE)

# Change response to either -1 or 1
credit_data[credit_data[,11] == 0,11] = -1
```

Judging by the naming scheme from the header, `R1` is the response and `Ai` are the predictors. The optimal C (or $\lambda$) value for a SVM classifier is found using a simple 2-nested grid search with 11 equidistant grid points (10 intervals) on each level. Initial search range is set to $C \in [1, 101]$. In-sample accuracy is used as the selection metric because the question specifically asks us not to separate data into training/validation sets. The code can be found in the corresponding section in the [Appendix](#searchC).

```{r, results = 'hide'}

result <- searchC(credit_data, range = c(1, 101), 11)

model <- result$model
a <- colSums(credit_data[model@SVindex, 1:10] * model@coef[[1]])

a0 <- sum(a*credit_data[1, 1:10]) - model@b

```
The final selected model is the one with the highest in-sample accuracy. It can be written as

```{r}
str = ""
for (i in 1:length(a)) {
    if (a[i] < 0) {
        num_str = paste('(', a[i], ')', sep = '')
    } else {
        num_str = paste(a[i])
    }
    
    if (i < length(a)) {
        str = paste(str, num_str, '*x_', i, 'j + ', sep = '')    
    } else {
        str = paste(str, num_str, '*x_', i, 'j + ', sep = '')
    }
}

if (a0 < 0) {
    num_str = paste('(', a0, ')', sep = '')
} else {
    num_str = paste(a0)
}

str = paste(str, num_str, ' = y_j', sep = '')

print(str)
```

# Appendix

## KSVM 2-Nested Grid Search for C value {#searchC}
A 2-nested grid search means a coarse grid is first used to get a general sense of where the maximum is located, then a finer grid is used to zoom in on the parameter value corresponding to the maximum. In this case, the maximum of interest is the in-sample prediction accuracy and the parameter is C.
```{r searchC, eval = FALSE, echo = TRUE}
require('kernlab')

# Compute accuracy
helper <- function(data, c_val) {
    rows = nrow(data)
    cols = ncol(data)
    
    model <- ksvm(as.matrix(data[,1:cols-1]), as.factor(data[,cols]), type="C-svc",kernel="vanilladot",C=c_val,scaled=TRUE)
    
    pred <- predict(model, data[,1:cols-1])
    
    ans <- list(c_val = c_val, acc = sum(pred == data[,cols])/rows, model = model)
    
    return(ans)
}

# data: data frame to be used for model building. Last column is the response.
# range: floating point in the form c(lower, upper). Search C in [lower, upper]. lower > 0!
# grid_n: number of grid points (including start and end points). Number of intervals = grid_n - 1.
# level: int indicating which level of grid search to perform (set to start at 1 initially)
searchC <- function(data, range = c(1, 101), grid_n, level = 1) {
    
    # grid search for maximum acc & corresponding index
    c_vals <- seq(range[1], range[2], length.out = grid_n)
    
    max_acc <- 0
    max_ind <- 1
    ans <- list()
    for (i in 1:grid_n) {
        ans <- helper(data, c_vals[i])
        
        if (ans$acc[1] > max_acc) {
            max_ind <- i
            max_acc <- ans$acc
        }
    }
    
    if (level < 2) {
        
        if (max_ind == 1) {
            range <- c(c_vals[max_ind], c_vals[max_ind + 2])
        } else if (max_ind == length(c_vals)) {
            range <- c(c_vals[max_ind - 2], c_vals[max_ind])
        } else {
            range <- c(c_vals[max_ind - 1], c_vals[max_ind + 1])
        }
        
        return(searchC(data, range, grid_n, level+1))
        
    } else {
        
        return(ans)
    }
}
```